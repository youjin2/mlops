version: "3"
services:
  batch-serving:
    build:
      context: .
    image: pystock-inference-api
    container_name: mlflow-serving
    ports:
      - "12000:12000"
    environment:
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
    volumes:
      - ./:/opt/api/
      - ../../../data/:/data/
    networks:
      - mlflow_default
    stdin_open: true
    tty: true

# connect to docker stack workbench network
networks:
  mlflow_default:
    external: true
